{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.1.0-cp37-cp37m-win_amd64.whl (355.8 MB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.11.3-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.28.1-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.1)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: h5py in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.14.1-py2.py3-none-any.whl (89 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acl6202e\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: gast, termcolor, absl-py\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7547 sha256=56ddf0ac9f2b93d813862fdbc74f170d81310118b0faa92ed49b6c266827c59f\n",
      "  Stored in directory: c:\\users\\acl6202e\\appdata\\local\\pip\\cache\\wheels\\21\\7f\\02\\420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=c315d081671e5a700092e99626bbe45a69d9890e8214193801f77409e2a63a0b\n",
      "  Stored in directory: c:\\users\\acl6202e\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121935 sha256=381df6879f976fa262f7bb91d26e63f424fc04ce88b0268e0c0ff07ec627a984\n",
      "  Stored in directory: c:\\users\\acl6202e\\appdata\\local\\pip\\cache\\wheels\\cc\\af\\1a\\498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built gast termcolor absl-py\n",
      "Installing collected packages: keras-preprocessing, astor, gast, opt-einsum, protobuf, keras-applications, tensorflow-estimator, grpcio, termcolor, absl-py, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, google-pasta, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.1.0 gast-0.2.2 google-auth-1.14.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why do we **preprocess** data when we build machine learning pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We preprocess data for two principle reasons:\n",
    "\n",
    "1. To transform the data to better suit a model's underlying assumptions.\n",
    "2. To format the data in the way a model expects.\n",
    "\n",
    "Today, we're concerned with this second reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What does the input to a neural network look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to neural networks are **vectors**. Each entry in the vector corresponds to a feature, which the net uses to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crucially, these vectors contain can contain only _numerical_ data. They _cannot_ contain string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good!\n",
    "good_input_row1 = [1.3, 2.2, 5.4, 5.8, 0]\n",
    "good_input_row2 = [1.3, 2.2, 5.4, 5.8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bad...\n",
    "bad_input_row1 = [1.3, 2.2, 5.4, 5.8, 'dog']\n",
    "bad_input_row2 = [1.3, 2.2, 5.4, 5.8, 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This poses a problem when we want to train a neural network on categorical data, such as the classic [Iris data set](https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read from: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the entries `iris-virginica`\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that all of our data is numerical..._Except_ for the data in that `class` column, which contains strings.\n",
    "\n",
    "The `class` column will contain one of three values:\n",
    "\n",
    "1. `iris-setosa`\n",
    "2. `iris-versicolour`\n",
    "3. `iris-virginica`\n",
    "\n",
    "As these are not numerical values, we can't use them to fit our nnet. To fix this, we must convert each class label to a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We do this via the following steps:\n",
    "\n",
    "1. **Label Encoding**. First, we convert the three possible classes to integer labels. E.g., `iris-setosa` will be `1`; `iris-versicolour`, `2`; and `iris-virginica`, `3`.\n",
    "2. **One-Hot Encoding**. Then, we set each row's `class` value to an _array_. This array will have a `1` in whichever slot corresponds to the integer label. E.g., after one-hot encoding, a row with the class `iris-setosa` will have the array `[1, 0, 0]`. A row with class `iris-virginica`, the array `[0, 0, 1]`; etc.\n",
    "\n",
    "In many cases, categories in the data sets you work with will already be label-encoded. In this case, you can apply one-hot encoding immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 0: Reformat data\n",
    "data = df.values\n",
    "X = data[:, 0:4]\n",
    "y = data[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for label, original_class in zip(encoded_y, y):\n",
    "    print('Original Class: ' + str(original_class))\n",
    "    print('Encoded Label: ' + str(label))\n",
    "    print('-' * 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that each of the original labels has been replaced with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "one_hot_y = to_categorical(encoded_y)\n",
    "one_hot_y"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
